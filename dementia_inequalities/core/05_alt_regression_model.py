# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/core/05_ml_custom_regression_model.ipynb.

# %% auto 0
__all__ = ['df_dem_plus', 'shift_log_normal_pdf', 'log_norm_mode', 'log_likelihood', 'log_prior', 'log_posterior',
           'metropolis_hastings']

# %% ../../nbs/core/05_ml_custom_regression_model.ipynb 4
import dementia_inequalities as proj 
from .. import const, log, utils, tools 
import adu_proj.utils as adutils 

# %% ../../nbs/core/05_ml_custom_regression_model.ipynb 5
import numpy as np 
import pandas as pd
from matplotlib import pyplot as plt
from sklearn import linear_model
from sklearn.model_selection import KFold, cross_val_score
from sklearn.linear_model import LinearRegression


# %% ../../nbs/core/05_ml_custom_regression_model.ipynb 8
def shift_log_normal_pdf(x:np.array, 
                         delta:int, # shift parameter
                         mu:int, # mean of the variables log
                         sigma:int): # standard deviation of the variables log
    if isinstance(delta, int) or isinstance(delta, float):
        delta = delta*np.ones(len(x)).T
    else:
        delta=delta.T
    mu = mu*np.ones(len(x)).T
    x_shift = np.subtract(x.squeeze(), delta.squeeze())
    norm_const = 1/((x_shift)*sigma*np.sqrt(2*np.pi))
    exp_part = np.exp(-(1/(2*sigma**2))*(np.log(np.subtract(x_shift.squeeze(), mu.squeeze()))**2))
    return norm_const*exp_part


def log_norm_mode(mu:int, # mean of the variables log
                  sigma:int): # standard deviation of the variables log
    return np.exp(mu - sigma**2)

# %% ../../nbs/core/05_ml_custom_regression_model.ipynb 12
# Define the log likelihood function for linear regression with log-normal error
def log_likelihood(params, X, y):
    beta_0 = params[0]
    beta = params[1:-2]
    mu = params[-2]
    sigma = params[-1]
    y_pred = np.dot(X, beta) + beta_0 # this is the shift 
    likelihood = shift_log_normal_pdf(y, delta=y_pred, mu=mu, sigma=sigma)
    return np.nansum(np.log(likelihood))

# Define the prior distribution for beta parameters, mu, and sigma
def log_prior(params):
    beta_0 = params[0]
    beta = params[1:-2]
    mu = params[-2]
    sigma = params[-1]
    #if 0 <= beta_0 and np.all(-10 < beta.all() < 10) and 0 <= mu < 10 and sigma > 0:
    if all(-10 < b < 10 for b in beta) and 0 <= beta_0 < 10 and -10 < mu < 10 and sigma > 0:
        return 0
    return -np.inf

# Define the log posterior distribution
def log_posterior(params, X, y):
    return log_likelihood(params, X, y) + log_prior(params)

# Define the Metropolis-Hastings algorithm
def metropolis_hastings(initial_params, proposal_sd, n_iter, X, y):
    params = initial_params
    accepted_params = [params]
    accepted_post = [log_posterior(params, X, y)]
    for _ in range(n_iter):
        proposed_params = params + np.random.normal(scale=proposal_sd, size=params.shape)
        log_alpha = log_posterior(proposed_params, X, y) - log_posterior(params, X, y)
        if np.log(np.random.rand()) < log_alpha:
            params = proposed_params
        accepted_params.append(params)
        accepted_post.append(log_posterior(params, X, y))
    return np.array(accepted_params), np.array(accepted_post)

# %% ../../nbs/core/05_ml_custom_regression_model.ipynb 21
df_dem_plus = pd.read_csv(const.output_path+'/df_dem_plus.csv')

df_dem_plus.head()
